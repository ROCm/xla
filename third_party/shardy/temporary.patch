diff --git a/shardy/dialect/sdy/transforms/export/BUILD b/shardy/dialect/sdy/transforms/export/BUILD
index 1732aec..53e24a0 100644
--- a/shardy/dialect/sdy/transforms/export/BUILD
+++ b/shardy/dialect/sdy/transforms/export/BUILD
@@ -52,7 +52,6 @@ cc_library(
         "//shardy/dialect/sdy/transforms/common:op_properties",
         "//shardy/dialect/sdy/transforms/propagation:op_sharding_rule_registry",
         "//shardy/dialect/sdy/transforms/propagation:sharding_projection",
-        "//shardy/dialect/sdy/transforms/propagation:utils",
         "@llvm-project//llvm:Support",
         "@llvm-project//mlir:FuncDialect",
         "@llvm-project//mlir:IR",
diff --git a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
index 7c21781..965084d 100644
--- a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
+++ b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
@@ -29,7 +29,6 @@ limitations under the License.
 #include "shardy/dialect/sdy/ir/utils.h"      // IWYU pragma: keep
 #include "shardy/dialect/sdy/transforms/propagation/op_sharding_rule_registry.h"
 #include "shardy/dialect/sdy/transforms/propagation/sharding_projection.h"
-#include "shardy/dialect/sdy/transforms/propagation/utils.h"
 #include "stablehlo/dialect/StablehloOps.h"  // IWYU pragma: keep
 
 namespace mlir {
@@ -119,31 +118,35 @@ bool hasCompatibleFactorShardings(const ShardingProjection& projection) {
 // Assumes factor shardings do not have overflow axes.
 // TODO(enver): Handle the case when some factor shardings have overflow axes.
 void insertExplicitReshards(Operation* op, const ShardingProjection& projection,
-                            UpdateTensorShardings updateTensorShardings,
                             IRRewriter& rewriter,
                             OpShardingRuleAttr shardingRule, StringRef meshName,
                             MeshAttr mesh) {
   rewriter.setInsertionPoint(op);
-  for (int operandIndex : updateTensorShardings.updateOperands.set_bits()) {
-    auto operand = op->getOperand(operandIndex);
+  for (const auto& [index, operand] : llvm::enumerate(op->getOperands())) {
     auto newTensorSharding =
-        projection.getOperand(operandIndex)
-            .createTensorShardingAttr(
-                mesh.getContext(), shardingRule.getOperandMapping(operandIndex),
-                shardingRule.getFactorSizes(), meshName, mesh);
+        projection.getOperand(index).createTensorShardingAttr(
+            mesh.getContext(), shardingRule.getOperandMapping(index),
+            shardingRule.getFactorSizes(), meshName, mesh);
+    if (newTensorSharding == getSharding(operand)) {
+      continue;
+    }
     auto reshardOp = rewriter.create<ReshardOp>(operand.getLoc(), operand,
                                                 newTensorSharding);
-    op->setOperand(operandIndex, reshardOp);
+    op->setOperand(index, reshardOp);
   }
 
   rewriter.setInsertionPointAfter(op);
-  for (int resultIndex : toSetBitsVector(updateTensorShardings.updateResults)) {
-    auto result = op->getResult(resultIndex);
-    auto newTensorSharding =
-        projection.getResult(resultIndex)
-            .createTensorShardingAttr(
-                mesh.getContext(), shardingRule.getResultMapping(resultIndex),
-                shardingRule.getFactorSizes(), meshName, mesh);
+  for (const auto& [result, tensorFactorShardings, tensorMapping] :
+       llvm::zip_equal(op->getResults(), projection.getResults(),
+                       shardingRule.getResultMappings())) {
+    // TODO(enver): The following logic is mostly shared between operands and
+    // results. Use a helper function, instead.
+    auto newTensorSharding = tensorFactorShardings.createTensorShardingAttr(
+        mesh.getContext(), tensorMapping, shardingRule.getFactorSizes(),
+        meshName, mesh);
+    if (newTensorSharding == getSharding(result)) {
+      continue;
+    }
     auto reshardOp = rewriter.create<ReshardOp>(result.getLoc(), result,
                                                 getSharding(result));
     rewriter.replaceAllUsesExcept(result, reshardOp, reshardOp);
@@ -160,8 +163,6 @@ struct InsertExplicitReshardsPass
     IRRewriter rewriter(funcOp);
     SymbolTable symbolTable(funcOp->getParentOfType<ModuleOp>());
     // TODO(enver): Handle data flow ops.
-    // TODO(enver): Handle cases func op result sharding does not match the
-    // sharding of returned value.
     funcOp.walk([&](Operation* op) {
       // TODO(enver): Check if data flow ops, data flow edge op, manual
       // computation op require extra check before creating sharding rule.
@@ -204,19 +205,15 @@ struct InsertExplicitReshardsPass
         return;
       }
 
-      UpdateTensorShardings updateTensorShardings(shardingRule.getNumOperands(),
-                                                  shardingRule.getNumResults());
-      for (const auto& [index, factorAxes] :
-           llvm::enumerate(shardingProjection.getGreatestCommonPrefixAxes(
-               shardingRule.getNumFactors()))) {
-        // TODO(enver): Add unit tests to test overflow axes are cleared after
-        // handling the case that some factors have overflow axes.
-        updateTensorShardings |= shardingProjection.updateSharding(
-            index, factorAxes, /*overflowAxes=*/{});
-      }
+      // TODO(enver): Instead of building a new projection, update and use the
+      // existing one.
+      ShardingProjection projection = ShardingProjection::build(
+          shardingProjection.getGreatestCommonPrefixAxes(
+              shardingRule.getNumFactors()),
+          shardingRule);
 
-      insertExplicitReshards(op, shardingProjection, updateTensorShardings,
-                             rewriter, shardingRule, *meshName, mesh);
+      insertExplicitReshards(op, projection, rewriter, shardingRule, *meshName,
+                             mesh);
 
       // TODO(enver): Remove sharding rules from ops.
     });
diff --git a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.cc b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.cc
index fc4f2f1..67b9ec4 100644
--- a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.cc
+++ b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.cc
@@ -20,7 +20,6 @@ limitations under the License.
 #include <tuple>
 
 #include "llvm/ADT/STLExtras.h"
-#include "llvm/ADT/Sequence.h"
 #include "llvm/ADT/SmallVector.h"
 #include "mlir/IR/Value.h"
 #include "mlir/Support/LLVM.h"
@@ -109,22 +108,8 @@ UpdateTensorShardings AggressiveFactorPropagation::propagateFactorShardings(
     return result;
   }
 
-  // We sort the factors based on:
-  // 1. larger source tensor size first
-  // 2. smaller source tensor index first
-  // 3. smaller factor index first
-  // Unstable sort is fine because there is no equality in the candidates.
-  // TODO(b/376233527): reevaluate this conflict resolution heuristic.
-  SmallVector<int64_t> sortedFactorIndices =
-      llvm::to_vector(llvm::seq<int64_t>(0, factorSizes.size()));
   SmallVector<TensorIndexSize> factorToSourceTensor =
       getFactorToSourceTensor(projection, factorSizes, axesPerFactor);
-  llvm::sort(sortedFactorIndices, [&](int64_t i, int64_t j) {
-    return std::forward_as_tuple(-factorToSourceTensor[i].size,
-                                 factorToSourceTensor[i].index, i) <
-           std::forward_as_tuple(-factorToSourceTensor[j].size,
-                                 factorToSourceTensor[j].index, j);
-  });
 
   // The propagation on each tensor is independent. This strategy can propagate
   // different shardings to different tensors along the same factor. Examples
@@ -132,24 +117,15 @@ UpdateTensorShardings AggressiveFactorPropagation::propagateFactorShardings(
   for (const auto& [tensorIndex, tensorFactorShardings] :
        llvm::enumerate(llvm::concat<const TensorFactorShardings>(
            projection.getOperands(), projection.getResults()))) {
-    const FactorIndexToSharding& factorIndexToSharding =
+    // Propagate the axes got in Step 1, and resolve conflicts within a factor.
+    FactorIndexToSharding newSharding =
         tensorFactorShardings.factorIndexToSharding;
-
-    // Propagate the axes got in Step 1, resolving conflicts between factors by
-    // following the order of preference in  `sortedFactorIndices`.
-    bool tensorUpdated = false;
-    for (int64_t factorIndex : sortedFactorIndices) {
-      auto factorShardingIt = factorIndexToSharding.find(factorIndex);
-      if (factorShardingIt == factorIndexToSharding.end()) {
-        continue;
-      }
-      const FactorSharding& factorSharding = factorShardingIt->second;
+    BitVector factorUpdated(factorSizes.size());
+    for (auto& [factorIndex, factorSharding] : newSharding) {
       SmallVector<AxisRefAttr> newAxes = axesPerFactor[factorIndex];
-
-      // Resolve conflicts within a factor.
       truncateAxesByRemovingConflicts(
           newAxes,
-          [&, factorIndex = factorIndex,
+          [&, factorIndex = factorIndex, &factorSharding = factorSharding,
            &tensorFactorShardings = tensorFactorShardings](
               AxisRefAttr axisRef, int64_t prevShardedSize) {
             return compatiblePrefixNoConflictsWithinFactor(
@@ -157,20 +133,34 @@ UpdateTensorShardings AggressiveFactorPropagation::propagateFactorShardings(
                 prevShardedSize, factorSizes[factorIndex], mesh);
           },
           mesh, conservativePropagation);
-      if (!isStrictPrefix(factorSharding.axisRefs, newAxes)) {
-        continue;
+      if (isStrictPrefix(factorSharding.axisRefs, newAxes)) {
+        factorSharding.axisRefs = newAxes;
+        factorUpdated.set(factorIndex);
       }
+    }
 
-      // Resolve conflicts (overlapping sharding axes) between factors.
-      //
-      // Note that we pass `factorIndexToSharding`, which might have been
-      // updated for a previous factor (previous iteration), thus we are
-      // checking for conflicts w.r.t. the updated state of this tensor.
+    SmallVector<int> sortedFactorIndices = toSetBitsVector(factorUpdated);
+    // We sort the factors based on:
+    // 1. larger source tensor size first
+    // 2. smaller source tensor index first
+    // 3. smaller factor index first
+    // Unstable sort is fine because there is no equality in the candidates.
+    llvm::sort(sortedFactorIndices, [&](int64_t i, int64_t j) {
+      return std::forward_as_tuple(-factorToSourceTensor[i].size,
+                                   factorToSourceTensor[i].index, i) <
+             std::forward_as_tuple(-factorToSourceTensor[j].size,
+                                   factorToSourceTensor[j].index, j);
+    });
+
+    // Resolve conflicts (overlapping sharding axes) between factors.
+    bool tensorUpdated = false;
+    for (const int64_t factorIndex : sortedFactorIndices) {
+      SmallVector<AxisRefAttr> newAxes = newSharding[factorIndex].axisRefs;
       truncateAxesByRemovingConflicts(
           newAxes,
           [&, factorIndex = factorIndex](AxisRefAttr axisRef, int64_t) {
             return compatiblePrefixNoConflictsAcrossFactors(
-                axisRef, factorIndexToSharding, factorIndex);
+                axisRef, newSharding, factorIndex);
           },
           mesh, conservativePropagation);
       tensorUpdated |=
diff --git a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.h b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.h
index 3f7e699..105ebad 100644
--- a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.h
+++ b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation.h
@@ -44,35 +44,28 @@ namespace sdy {
 // `BasicFactorPropagation` is conservative in terms of conflicts across
 // factors. The overlapped axis between factors cannot be propagated. This
 // strategy is more aggressive by allowing the overlapped axis being propagated
-// along different factors if there is no overlapped axis in the current
+// along different factors if there is no overlapped axis in the result
 // shardings.
 //
-// To resolve conflicts across factors, when there are multiple choices (that
-// cannot co-exist), we prefer the factor with the larger source tensor (the
-// tensor from which the factor sharding is propagated), as it's normally
-// beneficial to reshard the smaller tensor. If two factors have the same source
-// tensor size, we sort based on the source tensor index and finally factor
-// index.
-//
 // Let us take C = dot(A, B) as an example. F0 is the factor corresponding to a
 // non-contracting dimension of A. F1 corresponds to a non-contracting dimension
-// of B. F2 corresponds to a contracting dimension. '-' means that the tensor
+// of B. F2 corresponds to a contracting dimension. "-" means that the tensor
 // does not contain the factor.
 //
 //     F0    F1    F2
 // A  "a"    -
 // B   -
 // C        "a"    -
-// Case 1. Conflict with a single choice. `BasicFactorPropagation` propagates
-// nothing, while this strategy propagates "a" to B/F1.
+// Case 1. Fake conflict. `BasicFactorPropagation` propagates nothing, while
+// this strategy propagates "a" to B/F1.
 //
 //     F0    F1    F2
 // A  "a"    -
 // B   -    "a"
 // C               -
-// Case 2. Conflict with multiple choices. `BasicFactorPropagation` propagates
-// nothing, while this strategy propagates "a" to C/F1, since F1 is preferred
-// over F0 (tensor B is larger than A).
+// Case 2. Real conflict. Both `BasicFactorPropagation` and this strategy
+// propagate nothing. We can propagate "a" to C/F0 or C/F1, which is illegal
+// since "a" cannot be used twice in C.
 class AggressiveFactorPropagation : public BasicFactorPropagation {
  public:
   UpdateTensorShardings propagateFactorShardings(
diff --git a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation_test.cc b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation_test.cc
index cec5ff8..5fd66e8 100644
--- a/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation_test.cc
+++ b/shardy/dialect/sdy/transforms/propagation/aggressive_factor_propagation_test.cc
@@ -94,7 +94,7 @@ TEST_F(AggressiveFactorPropagationTest, RealAndFakeConflicts) {
       /*results=*/{
           {.factorIndexToSharding =
                {
-                   {0, {.axisRefs = {createAxis("a")}}},
+                   {0, {.axisRefs = {}}},
                    {1, {.axisRefs = {}}},
                    {2, {.axisRefs = {}, .overflowAxes = {createAxis("d")}}},
                    {3, {.axisRefs = {createAxis("c")}}},
@@ -108,8 +108,7 @@ TEST_F(AggressiveFactorPropagationTest, RealAndFakeConflicts) {
       });
 
   // Axis "a" may be propagated to the result along factors 0 or 1, which forms
-  // a real conflict. We prefer factor 0 because its source is the first operand
-  // (all tensors have the same size).
+  // a real conflict. Thus, we do not apply either of propagation choices.
   //
   // Other conflicts are fake. We can propagate other axes as much as possible.
   // Axes "c", "b", "e", "f", "g" can be propagated to the result along factors
@@ -321,7 +320,7 @@ TEST_F(AggressiveFactorPropagationTest, NewAxesConflict) {
       /*results=*/{
           {.factorIndexToSharding =
                {
-                   {0, {.axisRefs = {createAxis("a"), createAxis("b")}}},
+                   {0, {.axisRefs = {}}},
                    {1, {.axisRefs = {}, .isClosed = true}},
                    {2, {.axisRefs = {createAxis("c")}}},
                    {3, {.axisRefs = {createAxis("d")}}},
@@ -338,8 +337,8 @@ TEST_F(AggressiveFactorPropagationTest, NewAxesConflict) {
       });
 
   // “a” can be propagated to the Result 0 along either Factor 0 or Factor 2.
-  // This strategy prefers factor 0 because its source is the first operand
-  // (all tensors have the same size).
+  // This strategy truncate “a” for both F0 and F2 in Result 0. Namely, this
+  // strategy does not resolve real conflicts across factors.
   auto [updateOperands, updateResults] =
       propagateFactorShardings(projection, 4);
   EXPECT_THAT(toSetBitsVector(updateOperands), ElementsAre(1, 2));
diff --git a/shardy/dialect/sdy/transforms/propagation/sharding_projection.cc b/shardy/dialect/sdy/transforms/propagation/sharding_projection.cc
index a22b055..99ea345 100644
--- a/shardy/dialect/sdy/transforms/propagation/sharding_projection.cc
+++ b/shardy/dialect/sdy/transforms/propagation/sharding_projection.cc
@@ -84,26 +84,6 @@ bool TensorFactorShardings::expandShardingAxes(int64_t factorIndex,
   return true;
 }
 
-bool TensorFactorShardings::updateShardingAxes(
-    int64_t factorIndex, ArrayRef<AxisRefAttr> newAxes,
-    ArrayRef<AxisRefAttr> newOverflowAxes) {
-  auto factorShardingIt = factorIndexToSharding.find(factorIndex);
-  if (factorShardingIt == factorIndexToSharding.end()) {
-    return false;
-  }
-
-  SmallVector<AxisRefAttr>& oldAxes = factorShardingIt->second.axisRefs;
-  SmallVector<AxisRefAttr>& oldOverflowAxes =
-      factorShardingIt->second.overflowAxes;
-  if (oldAxes == newAxes && oldOverflowAxes == newOverflowAxes) {
-    return false;
-  }
-
-  oldAxes = llvm::to_vector(newAxes);
-  oldOverflowAxes = llvm::to_vector(newOverflowAxes);
-  return true;
-}
-
 namespace {
 
 // Adds all axes in `axes` to `dimSharding`.
@@ -191,21 +171,6 @@ UpdateTensorShardings ShardingProjection::expandSharding(
   return result;
 }
 
-UpdateTensorShardings ShardingProjection::updateSharding(
-    int64_t factorIndex, ArrayRef<AxisRefAttr> newAxes,
-    ArrayRef<AxisRefAttr> newOverflowAxes) {
-  UpdateTensorShardings result(getNumOperands(), getNumResults());
-  for (auto [i, tensor] : llvm::enumerate(operands)) {
-    result.updateOperands[i] =
-        tensor.updateShardingAxes(factorIndex, newAxes, newOverflowAxes);
-  }
-  for (auto [i, tensor] : llvm::enumerate(results)) {
-    result.updateResults[i] =
-        tensor.updateShardingAxes(factorIndex, newAxes, newOverflowAxes);
-  }
-  return result;
-}
-
 namespace {
 
 // Holds the size of an axis ref, and its pre-size if it's a sub-axis.
@@ -406,8 +371,9 @@ ShardingProjection ShardingProjection::build(Operation* op,
                shardingRule, mesh);
 }
 
-ShardingProjection ShardingProjection::build(AxesPerFactorRef axesPerFactorRef,
-                                             OpShardingRuleAttr shardingRule) {
+ShardingProjection ShardingProjection::build(
+    AxesPerFactorRef axesPerFactorRef,
+    OpShardingRuleAttr shardingRule) {
   ShardingProjection projection;
   for (const auto& operandMapping : shardingRule.getOperandMappings()) {
     projection.operands.push_back(
diff --git a/shardy/dialect/sdy/transforms/propagation/sharding_projection.h b/shardy/dialect/sdy/transforms/propagation/sharding_projection.h
index 299f57b..d82cb3d 100644
--- a/shardy/dialect/sdy/transforms/propagation/sharding_projection.h
+++ b/shardy/dialect/sdy/transforms/propagation/sharding_projection.h
@@ -94,17 +94,6 @@ struct TensorFactorShardings {
   // Returns if the sharding axes have been expanded.
   bool expandShardingAxes(int64_t factorIndex, ArrayRef<AxisRefAttr> newAxes);
 
-  // Updates the factor sharding at `factorIndex` with new sharding axes
-  // `newAxes` and new overflow axes `newOverflowAxes` if
-  // 1. this tensor is associated with that factor, and
-  // 2. the existing axes are different than the new ones.
-  //
-  // Assumes either new overflow axes are empty or the factor is non-minor-most.
-  //
-  // Returns if the factor sharding has been updated.
-  bool updateShardingAxes(int64_t factorIndex, ArrayRef<AxisRefAttr> newAxes,
-                          ArrayRef<AxisRefAttr> newOverflowAxes);
-
   // Creates a `TensorShardingAttr` by projecting the factor shardings in
   // this `TensorFactorShardings` to dimension shardings w.r.t. to
   // `tensorMapping`.
@@ -125,12 +114,6 @@ struct UpdateTensorShardings {
   UpdateTensorShardings(int64_t numOperands, int64_t numResults)
       : updateOperands(BitVector(numOperands)),
         updateResults(BitVector(numResults)) {}
-
-  UpdateTensorShardings& operator|=(const UpdateTensorShardings& other) {
-    updateOperands |= other.updateOperands;
-    updateResults |= other.updateResults;
-    return *this;
-  }
 };
 
 // The sharding projection holds information about how factors (rather than
@@ -201,14 +184,6 @@ class ShardingProjection {
   UpdateTensorShardings expandSharding(int64_t factorIndex,
                                        ArrayRef<AxisRefAttr> newAxes);
 
-  // Updates the shardings of all tensors that are associated with
-  // `factorIndex` to be `newAxes` and `newOverflowAxes` for that factor. Keep
-  // it open/close if it is already open/close. Returns two BitVectors
-  // indicating whether the operands and results have been updated.
-  UpdateTensorShardings updateSharding(int64_t factorIndex,
-                                       ArrayRef<AxisRefAttr> newAxes,
-                                       ArrayRef<AxisRefAttr> newOverflowAxes);
-
   // Builds a `ShardingProjection` for the given operand and result shardings,
   // w.r.t. the given `shardingRule`.
   static ShardingProjection build(ArrayRef<TensorShardingAttr> operandShardings,
diff --git a/shardy/dialect/sdy/transforms/propagation/sharding_projection_test.cc b/shardy/dialect/sdy/transforms/propagation/sharding_projection_test.cc
index 20c914c..40526ed 100644
--- a/shardy/dialect/sdy/transforms/propagation/sharding_projection_test.cc
+++ b/shardy/dialect/sdy/transforms/propagation/sharding_projection_test.cc
@@ -1084,54 +1084,6 @@ TEST_F(TensorFactorShardingsTest, ExpandShardingAxes_DoesNotExpand) {
       /*factorIndex=*/0, {createAxis("c"), createAxis("b")}));
 }
 
-TEST_F(TensorFactorShardingsTest, UpdateShardingAxes_Updates) {
-  const std::string program = R"mlir(
-    sdy.mesh @mesh = <["a"=2, "b"=4, "c"=4]>
-
-    func.func @main(%arg0: tensor<4x4xf32>) -> tensor<16xf32> {
-      %0 = stablehlo.reshape %arg0 : (tensor<4x4xf32>) -> tensor<16xf32>
-      return %0 : tensor<16xf32>
-    })mlir";
-
-  OwningOpRef<ModuleOp> module = parseSourceString<ModuleOp>(program, &context);
-  ASSERT_TRUE(module);
-  auto op = getFirstOp<stablehlo::ReshapeOp>(module.get());
-  OpShardingRuleAttr shardingRule = getOrCreateShardingRule(op);
-  TensorFactorShardings factorShardings{
-      .factorIndexToSharding = {{0, {.axisRefs = {createAxis("b")}}},
-                                {1, {.axisRefs = {createAxis("a")}}}}};
-
-  EXPECT_TRUE(factorShardings.updateShardingAxes(
-      /*factorIndex=*/0, /*axisRefs=*/{createAxis("c")},
-      /*newOverflowAxes=*/{}));
-
-  verifyShardingAttrsMatch(
-      factorShardings.createTensorShardingAttr(
-          &context, shardingRule.getOperandMapping(0),
-          shardingRule.getFactorSizes(), kMeshName, getMeshAttr(module.get())),
-      createTensorSharding(
-          /*dimShardings=*/{openDimSharding({createAxis("c")}),
-                            openDimSharding({createAxis("a")})}));
-
-  verifyShardingAttrsMatch(
-      factorShardings.createTensorShardingAttr(
-          &context, shardingRule.getResultMapping(0),
-          shardingRule.getFactorSizes(), kMeshName, getMeshAttr(module.get())),
-      createTensorSharding(
-          /*dimShardings=*/{
-              openDimSharding({createAxis("c"), createAxis("a")})}));
-}
-
-TEST_F(TensorFactorShardingsTest, UpdateShardingAxes_DoesNotUpdate) {
-  TensorFactorShardings factorShardings{
-      .factorIndexToSharding = {{0, {.axisRefs = {createAxis("b")}}},
-                                {1, {.axisRefs = {createAxis("a")}}}}};
-
-  EXPECT_FALSE(factorShardings.updateShardingAxes(
-      /*factorIndex=*/0, /*newAxes=*/{createAxis("b")},
-      /*newOverflowAxes=*/{}));
-}
-
 //===----------------------------------------------------------------------===//
 // Tests for ShardingProjection::getGreatestCommonPrefixAxes
 //===----------------------------------------------------------------------===//
diff --git a/shardy/dialect/sdy/transforms/propagation/test/aggressive_propagation.mlir b/shardy/dialect/sdy/transforms/propagation/test/aggressive_propagation.mlir
index fc7a805..f109bb1 100644
--- a/shardy/dialect/sdy/transforms/propagation/test/aggressive_propagation.mlir
+++ b/shardy/dialect/sdy/transforms/propagation/test/aggressive_propagation.mlir
@@ -3,7 +3,6 @@
 sdy.mesh @empty_mesh = <[]>
 sdy.mesh @mesh_a_2_b_2 = <["a"=2, "b"=2]>
 sdy.mesh @mesh_a_2_b_2_c_2 = <["a"=2, "b"=2, "c"=2]>
-sdy.mesh @mesh_a_2_b_2_c_2_d_2 = <["a"=2, "b"=2, "c"=2, "d"=2]>
 
 // CHECK-LABEL: func @no_conflict(
 // CHECK-SAME:      %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2, [{"a"}, {"b"}]>},
@@ -28,7 +27,7 @@ func.func @no_conflict(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mes
 func.func @fake_conflict_between_two_non_contracting_dims(%arg0: tensor<256x512xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2, [{"a", ?}, {?}]>},
                                                           %arg1: tensor<128x512xf32>)
           -> (tensor<256x128xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2, [{?}, {"a", ?}]>}) {
-  // CHECK-NEXT: stablehlo.dot_general %arg0, %arg1
+  // CHECK-NEXT: %0 = stablehlo.dot_general %arg0, %arg1
   // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2, [{?}, {"a", ?}]>]>}
   %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2, [{?}, {"a", ?}]>]>} :
     (tensor<256x512xf32>, tensor<128x512xf32>) -> tensor<256x128xf32>
@@ -41,7 +40,7 @@ func.func @fake_conflict_between_two_non_contracting_dims(%arg0: tensor<256x512x
 // CHECK-SAME:  -> (tensor<256x128xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {"c", "b", ?}]>}) {
 func.func @fake_conflict_between_contracting_and_non_contracting_dims(%arg0: tensor<256x512xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {"b", "c"}]>},
                                                                       %arg1: tensor<128x512xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"c", "b"}, {"a"}]>}) -> tensor<256x128xf32> {
-  // CHECK-NEXT: stablehlo.dot_general %arg0, %arg1
+  // CHECK-NEXT: %0 = stablehlo.dot_general %arg0, %arg1
   // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", ?}, {"c", "b", ?}]>]>}
   %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [1] :
     (tensor<256x512xf32>, tensor<128x512xf32>) -> tensor<256x128xf32>
@@ -54,19 +53,31 @@ func.func @fake_conflict_between_contracting_and_non_contracting_dims(%arg0: ten
 // CHECK-SAME:  -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b", "c", ?}]>}) {
 func.func @fake_conflict_closed_dims(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b"}]>},
                                      %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{}, {"b", "c", ?}]>}) -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.add %arg0, %arg1
+  // CHECK-NEXT: %0 = stablehlo.add %arg0, %arg1
   // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b", "c", ?}]>]>}
   %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
   return %0 : tensor<8x8xf32>
 }
 
+// CHECK-LABEL: func @real_conflict_across_factors(
+// CHECK-SAME:      %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {?}]>},
+// CHECK-SAME:      %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a"}]>})
+// CHECK-SAME:  -> tensor<8x8xf32> {
+func.func @real_conflict_across_factors(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {?}]>},
+                                        %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a"}]>}) -> tensor<8x8xf32> {
+  // CHECK-NEXT: %0 = stablehlo.add %arg0, %arg1
+  // CHECK-NOT:    sdy.sharding
+  %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
+  return %0 : tensor<8x8xf32>
+}
+
 // CHECK-LABEL: func @real_conflict_within_a_factor(
 // CHECK-SAME:      %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {}]>},
 // CHECK-SAME:      %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"b", ?}, {}]>})
 // CHECK-SAME:  -> tensor<8x8xf32> {
 func.func @real_conflict_within_a_factor(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {}]>},
                                          %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"b", ?}, {}]>}) -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.add %arg0, %arg1
+  // CHECK-NEXT: %0 = stablehlo.add %arg0, %arg1
   // CHECK-NOT:    sdy.sharding
   %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
   return %0 : tensor<8x8xf32>
@@ -78,7 +89,7 @@ func.func @real_conflict_within_a_factor(%arg0: tensor<8x8xf32> {sdy.sharding =
 // CHECK-SAME:  -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a", ?}]>}) {
 func.func @real_and_fake_conflicts(%arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {?}]>},
                                    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"b", ?}, {"a", ?}]>}) -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.add %arg0, %arg1
+  // CHECK-NEXT: %0 = stablehlo.add %arg0, %arg1
   // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{?}, {"a", ?}]>]>}
   %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
   return %0 : tensor<8x8xf32>
@@ -94,59 +105,3 @@ func.func @empty_mesh_replaced_closed_dim_respected(
   %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
   return %0 : tensor<8x8xf32>
 }
-
-// CHECK-LABEL: func @real_conflict_across_factors_same_tensors_size(
-// CHECK-SAME:      %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {?}]>},
-// CHECK-SAME:      %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a"}]>})
-func.func @real_conflict_across_factors_same_tensors_size(
-    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {?}]>},
-    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a"}]>})
-    -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.add %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", ?}, {?}]>]>}
-  %0 = stablehlo.add %arg0, %arg1 : tensor<8x8xf32>
-  return %0 : tensor<8x8xf32>
-}
-
-// CHECK-LABEL: func @real_conflict_across_factors_diff_tensors_size(
-// CHECK-SAME:      %arg0: tensor<8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", "b"}, {?}]>},
-// CHECK-SAME:      %arg1: tensor<4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a", "c"}]>})
-func.func @real_conflict_across_factors_diff_tensors_size(
-    %arg0: tensor<8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", "b"}, {?}]>},
-    %arg1: tensor<4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"a", "c"}]>})
-    -> tensor<8x16xf32> {
-  // CHECK-NEXT: stablehlo.dot_general %arg0, %arg1
-  // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{?}, {"a", "c", ?}]>]>}
-  %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0] :
-    (tensor<8x4xf32>, tensor<4x16xf32>) -> tensor<8x16xf32>
-  return %0 : tensor<8x16xf32>
-}
-
-// CHECK-LABEL: func @partial_conflict_across_factors(
-// CHECK-SAME:      %arg0: tensor<2x8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {"b"}, {}]>},
-// CHECK-SAME:      %arg1: tensor<2x4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", "b", ?}, {?}, {?}]>})
-func.func @partial_conflict_across_factors(
-    %arg0: tensor<2x8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a"}, {"b"}, {}]>},
-    %arg1: tensor<2x4x16xf32>)
-    -> tensor<2x8x16xf32> {
-  // CHECK-NEXT: stablehlo.dot_general %arg0, %arg1
-  // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", "b"}, {}, {}]>]>}
-  %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1]
-    {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", "b"}, {}, {}]>]>} :
-    (tensor<2x8x4xf32>, tensor<2x4x16xf32>) -> tensor<2x8x16xf32>
-  return %0 : tensor<2x8x16xf32>
-}
-
-// CHECK-LABEL: func @multiple_conflicts_across_factors(
-// CHECK-SAME:      %arg0: tensor<2x8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2_d_2, [{"d", ?}, {"a", "b"}, {?}]>},
-// CHECK-SAME:      %arg1: tensor<2x4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2_d_2, [{?}, {"d", "c"}, {"b", "a"}]>})
-func.func @multiple_conflicts_across_factors(
-    %arg0: tensor<2x8x4xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2_d_2, [{?}, {"a", "b"}, {?}]>},
-    %arg1: tensor<2x4x16xf32> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2_d_2, [{?}, {"d", "c"}, {"b", "a"}]>})
-    -> tensor<2x8x16xf32> {
-  // CHECK-NEXT: stablehlo.dot_general %arg0, %arg1
-  // CHECK-SAME:   {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2_d_2, [{"d"}, {?}, {"b", "a", ?}]>]>}
-  %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1]
-    {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2_d_2, [{"d"}, {?}, {?}]>]>} :
-    (tensor<2x8x4xf32>, tensor<2x4x16xf32>) -> tensor<2x8x16xf32>
-  return %0 : tensor<2x8x16xf32>
-}
diff --git a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline_data_flow_edges.mlir b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline_data_flow_edges.mlir
index 8b45c62..beced74 100644
--- a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline_data_flow_edges.mlir
+++ b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline_data_flow_edges.mlir
@@ -120,15 +120,14 @@ func.func @case_multiple_results_different_sharding(
 
 // Make sure we account for when an axis is used in another dimension when
 // finding the most compatible major sharding axes.
-// In this case we prefer the sharding of %arg1, because both %arg1 and %arg2
-// have the same size.
 // CHECK-LABEL: func @case_multiple_dim_most_compatible(
 // CHECK-SAME:      %arg0: tensor<i32>,
 // CHECK-SAME:      %arg1: tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b", ?}]>},
 // CHECK-SAME:      %arg2: tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", "b", ?}, {?}]>}
-// CHECK-SAME:      -> (tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b", ?}]>})
+// CHECK-SAME:      -> (tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", ?}, {?}]>})
 func.func @case_multiple_dim_most_compatible(
   %arg0: tensor<i32>,
+
   %arg1: tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{?}, {"b", ?}]>},
   %arg2: tensor<8x8xi64> {sdy.sharding = #sdy.sharding<@mesh_a_2_b_2_c_2, [{"a", "b", ?}, {?}]>}
   ) -> (tensor<8x8xi64>) {
@@ -136,7 +135,7 @@ func.func @case_multiple_dim_most_compatible(
     stablehlo.return %arg1 : tensor<8x8xi64>
   }, {
     stablehlo.return %arg2 : tensor<8x8xi64>
-  // CHECK: }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", ?}, {"b", ?}]>]>} :
+  // CHECK: }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh_a_2_b_2_c_2, [{"a", ?}, {?}]>]>} :
   }) : (tensor<i32>) -> (tensor<8x8xi64>)
   return %0 : tensor<8x8xi64>
 }
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index babbda4..5d1288a 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,263 +1,4 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/clang/include/clang/AST/DeclTemplate.h b/clang/include/clang/AST/DeclTemplate.h
---- a/clang/include/clang/AST/DeclTemplate.h
-+++ b/clang/include/clang/AST/DeclTemplate.h
-@@ -857,16 +857,6 @@
-   /// \endcode
-   bool isMemberSpecialization() const { return Common.getInt(); }
- 
--  /// Determines whether any redeclaration of this template was
--  /// a specialization of a member template.
--  bool hasMemberSpecialization() const {
--    for (const auto *D : redecls()) {
--      if (D->isMemberSpecialization())
--        return true;
--    }
--    return false;
--  }
--
-   /// Note that this member template is a specialization.
-   void setMemberSpecialization() {
-     assert(!isMemberSpecialization() && "already a member specialization");
-@@ -1965,13 +1955,7 @@
-   /// specialization which was specialized by this.
-   llvm::PointerUnion<ClassTemplateDecl *,
-                      ClassTemplatePartialSpecializationDecl *>
--  getSpecializedTemplateOrPartial() const {
--    if (const auto *PartialSpec =
--            SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization *>())
--      return PartialSpec->PartialSpecialization;
--
--    return SpecializedTemplate.get<ClassTemplateDecl*>();
--  }
-+  getSpecializedTemplateOrPartial() const;
- 
-   /// Retrieve the set of template arguments that should be used
-   /// to instantiate members of the class template or class template partial
-@@ -2208,17 +2192,6 @@
-     return InstantiatedFromMember.getInt();
-   }
- 
--  /// Determines whether any redeclaration of this this class template partial
--  /// specialization was a specialization of a member partial specialization.
--  bool hasMemberSpecialization() const {
--    for (const auto *D : redecls()) {
--      if (cast<ClassTemplatePartialSpecializationDecl>(D)
--              ->isMemberSpecialization())
--        return true;
--    }
--    return false;
--  }
--
-   /// Note that this member template is a specialization.
-   void setMemberSpecialization() { return InstantiatedFromMember.setInt(true); }
- 
-@@ -2740,13 +2713,7 @@
-   /// Retrieve the variable template or variable template partial
-   /// specialization which was specialized by this.
-   llvm::PointerUnion<VarTemplateDecl *, VarTemplatePartialSpecializationDecl *>
--  getSpecializedTemplateOrPartial() const {
--    if (const auto *PartialSpec =
--            SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization *>())
--      return PartialSpec->PartialSpecialization;
--
--    return SpecializedTemplate.get<VarTemplateDecl *>();
--  }
-+  getSpecializedTemplateOrPartial() const;
- 
-   /// Retrieve the set of template arguments that should be used
-   /// to instantiate the initializer of the variable template or variable
-@@ -2980,18 +2947,6 @@
-     return InstantiatedFromMember.getInt();
-   }
- 
--  /// Determines whether any redeclaration of this this variable template
--  /// partial specialization was a specialization of a member partial
--  /// specialization.
--  bool hasMemberSpecialization() const {
--    for (const auto *D : redecls()) {
--      if (cast<VarTemplatePartialSpecializationDecl>(D)
--              ->isMemberSpecialization())
--        return true;
--    }
--    return false;
--  }
--
-   /// Note that this member template is a specialization.
-   void setMemberSpecialization() { return InstantiatedFromMember.setInt(true); }
- 
-@@ -3164,6 +3119,9 @@
-     return makeSpecIterator(getSpecializations(), true);
-   }
- 
-+  /// Merge \p Prev with our RedeclarableTemplateDecl::Common.
-+  void mergePrevDecl(VarTemplateDecl *Prev);
-+
-   // Implement isa/cast/dyncast support
-   static bool classof(const Decl *D) { return classofKind(D->getKind()); }
-   static bool classofKind(Kind K) { return K == VarTemplate; }
-diff -ruN --strip-trailing-cr a/clang/lib/AST/ASTImporter.cpp b/clang/lib/AST/ASTImporter.cpp
---- a/clang/lib/AST/ASTImporter.cpp
-+++ b/clang/lib/AST/ASTImporter.cpp
-@@ -6190,7 +6190,8 @@
- ExpectedDecl ASTNodeImporter::VisitClassTemplateSpecializationDecl(
-                                           ClassTemplateSpecializationDecl *D) {
-   ClassTemplateDecl *ClassTemplate;
--  if (Error Err = importInto(ClassTemplate, D->getSpecializedTemplate()))
-+  if (Error Err = importInto(ClassTemplate,
-+                             D->getSpecializedTemplate()->getCanonicalDecl()))
-     return std::move(Err);
- 
-   // Import the context of this declaration.
-diff -ruN --strip-trailing-cr a/clang/lib/AST/Decl.cpp b/clang/lib/AST/Decl.cpp
---- a/clang/lib/AST/Decl.cpp
-+++ b/clang/lib/AST/Decl.cpp
-@@ -2708,7 +2708,7 @@
-     if (isTemplateInstantiation(VDTemplSpec->getTemplateSpecializationKind())) {
-       auto From = VDTemplSpec->getInstantiatedFrom();
-       if (auto *VTD = From.dyn_cast<VarTemplateDecl *>()) {
--        while (!VTD->hasMemberSpecialization()) {
-+        while (!VTD->isMemberSpecialization()) {
-           if (auto *NewVTD = VTD->getInstantiatedFromMemberTemplate())
-             VTD = NewVTD;
-           else
-@@ -2718,7 +2718,7 @@
-       }
-       if (auto *VTPSD =
-               From.dyn_cast<VarTemplatePartialSpecializationDecl *>()) {
--        while (!VTPSD->hasMemberSpecialization()) {
-+        while (!VTPSD->isMemberSpecialization()) {
-           if (auto *NewVTPSD = VTPSD->getInstantiatedFromMember())
-             VTPSD = NewVTPSD;
-           else
-@@ -2732,7 +2732,7 @@
-   // If this is the pattern of a variable template, find where it was
-   // instantiated from. FIXME: Is this necessary?
-   if (VarTemplateDecl *VTD = VD->getDescribedVarTemplate()) {
--    while (!VTD->hasMemberSpecialization()) {
-+    while (!VTD->isMemberSpecialization()) {
-       if (auto *NewVTD = VTD->getInstantiatedFromMemberTemplate())
-         VTD = NewVTD;
-       else
-@@ -4153,7 +4153,7 @@
-   if (FunctionTemplateDecl *Primary = getPrimaryTemplate()) {
-     // If we hit a point where the user provided a specialization of this
-     // template, we're done looking.
--    while (!ForDefinition || !Primary->hasMemberSpecialization()) {
-+    while (!ForDefinition || !Primary->isMemberSpecialization()) {
-       if (auto *NewPrimary = Primary->getInstantiatedFromMemberTemplate())
-         Primary = NewPrimary;
-       else
-@@ -4170,7 +4170,7 @@
-   if (FunctionTemplateSpecializationInfo *Info
-         = TemplateOrSpecialization
-             .dyn_cast<FunctionTemplateSpecializationInfo*>()) {
--    return Info->getTemplate();
-+    return Info->getTemplate()->getMostRecentDecl();
-   }
-   return nullptr;
- }
-diff -ruN --strip-trailing-cr a/clang/lib/AST/DeclCXX.cpp b/clang/lib/AST/DeclCXX.cpp
---- a/clang/lib/AST/DeclCXX.cpp
-+++ b/clang/lib/AST/DeclCXX.cpp
-@@ -2030,7 +2030,7 @@
-   if (auto *TD = dyn_cast<ClassTemplateSpecializationDecl>(this)) {
-     auto From = TD->getInstantiatedFrom();
-     if (auto *CTD = From.dyn_cast<ClassTemplateDecl *>()) {
--      while (!CTD->hasMemberSpecialization()) {
-+      while (!CTD->isMemberSpecialization()) {
-         if (auto *NewCTD = CTD->getInstantiatedFromMemberTemplate())
-           CTD = NewCTD;
-         else
-@@ -2040,7 +2040,7 @@
-     }
-     if (auto *CTPSD =
-             From.dyn_cast<ClassTemplatePartialSpecializationDecl *>()) {
--      while (!CTPSD->hasMemberSpecialization()) {
-+      while (!CTPSD->isMemberSpecialization()) {
-         if (auto *NewCTPSD = CTPSD->getInstantiatedFromMemberTemplate())
-           CTPSD = NewCTPSD;
-         else
-diff -ruN --strip-trailing-cr a/clang/lib/AST/DeclTemplate.cpp b/clang/lib/AST/DeclTemplate.cpp
---- a/clang/lib/AST/DeclTemplate.cpp
-+++ b/clang/lib/AST/DeclTemplate.cpp
-@@ -993,7 +993,17 @@
-   if (const auto *PartialSpec =
-           SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization*>())
-     return PartialSpec->PartialSpecialization->getSpecializedTemplate();
--  return SpecializedTemplate.get<ClassTemplateDecl*>();
-+  return SpecializedTemplate.get<ClassTemplateDecl *>()->getMostRecentDecl();
-+}
-+
-+llvm::PointerUnion<ClassTemplateDecl *,
-+                   ClassTemplatePartialSpecializationDecl *>
-+ClassTemplateSpecializationDecl::getSpecializedTemplateOrPartial() const {
-+  if (const auto *PartialSpec =
-+          SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization *>())
-+    return PartialSpec->PartialSpecialization->getMostRecentDecl();
-+
-+  return SpecializedTemplate.get<ClassTemplateDecl *>()->getMostRecentDecl();
- }
- 
- SourceRange
-@@ -1283,6 +1293,39 @@
-   return CommonPtr;
- }
- 
-+void VarTemplateDecl::mergePrevDecl(VarTemplateDecl *Prev) {
-+  // If we haven't created a common pointer yet, then it can just be created
-+  // with the usual method.
-+  if (!getCommonPtrInternal())
-+    return;
-+
-+  Common *ThisCommon = static_cast<Common *>(getCommonPtrInternal());
-+  Common *PrevCommon = nullptr;
-+  SmallVector<VarTemplateDecl *, 8> PreviousDecls;
-+  for (; Prev; Prev = Prev->getPreviousDecl()) {
-+    if (CommonBase *C = Prev->getCommonPtrInternal()) {
-+      PrevCommon = static_cast<Common *>(C);
-+      break;
-+    }
-+    PreviousDecls.push_back(Prev);
-+  }
-+
-+  // If the previous redecl chain hasn't created a common pointer yet, then just
-+  // use this common pointer.
-+  if (!PrevCommon) {
-+    for (auto *D : PreviousDecls)
-+      D->setCommonPtr(ThisCommon);
-+    return;
-+  }
-+
-+  // Ensure we don't leak any important state.
-+  assert(ThisCommon->Specializations.empty() &&
-+         ThisCommon->PartialSpecializations.empty() &&
-+         "Can't merge incompatible declarations!");
-+
-+  setCommonPtr(PrevCommon);
-+}
-+
- VarTemplateSpecializationDecl *
- VarTemplateDecl::findSpecialization(ArrayRef<TemplateArgument> Args,
-                                     void *&InsertPos) {
-@@ -1405,7 +1448,16 @@
-   if (const auto *PartialSpec =
-           SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization *>())
-     return PartialSpec->PartialSpecialization->getSpecializedTemplate();
--  return SpecializedTemplate.get<VarTemplateDecl *>();
-+  return SpecializedTemplate.get<VarTemplateDecl *>()->getMostRecentDecl();
-+}
-+
-+llvm::PointerUnion<VarTemplateDecl *, VarTemplatePartialSpecializationDecl *>
-+VarTemplateSpecializationDecl::getSpecializedTemplateOrPartial() const {
-+  if (const auto *PartialSpec =
-+          SpecializedTemplate.dyn_cast<SpecializedPartialSpecialization *>())
-+    return PartialSpec->PartialSpecialization->getMostRecentDecl();
-+
-+  return SpecializedTemplate.get<VarTemplateDecl *>()->getMostRecentDecl();
- }
- 
- SourceRange VarTemplateSpecializationDecl::getSourceRange() const {
 diff -ruN --strip-trailing-cr a/clang/lib/AST/Type.cpp b/clang/lib/AST/Type.cpp
 --- a/clang/lib/AST/Type.cpp
 +++ b/clang/lib/AST/Type.cpp
@@ -300,263 +41,6 @@ diff -ruN --strip-trailing-cr a/clang/lib/AST/Type.cpp b/clang/lib/AST/Type.cpp
    }
  
    // Non-pointer types.
-diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaDecl.cpp b/clang/lib/Sema/SemaDecl.cpp
---- a/clang/lib/Sema/SemaDecl.cpp
-+++ b/clang/lib/Sema/SemaDecl.cpp
-@@ -4694,8 +4694,10 @@
- 
-   // Keep a chain of previous declarations.
-   New->setPreviousDecl(Old);
--  if (NewTemplate)
-+  if (NewTemplate) {
-+    NewTemplate->mergePrevDecl(OldTemplate);
-     NewTemplate->setPreviousDecl(OldTemplate);
-+  }
- 
-   // Inherit access appropriately.
-   New->setAccess(Old->getAccess());
-diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaInit.cpp b/clang/lib/Sema/SemaInit.cpp
---- a/clang/lib/Sema/SemaInit.cpp
-+++ b/clang/lib/Sema/SemaInit.cpp
-@@ -9954,7 +9954,7 @@
-     auto SynthesizeAggrGuide = [&](InitListExpr *ListInit) {
-       auto *Pattern = Template;
-       while (Pattern->getInstantiatedFromMemberTemplate()) {
--        if (Pattern->hasMemberSpecialization())
-+        if (Pattern->isMemberSpecialization())
-           break;
-         Pattern = Pattern->getInstantiatedFromMemberTemplate();
-       }
-diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaTemplateInstantiate.cpp b/clang/lib/Sema/SemaTemplateInstantiate.cpp
---- a/clang/lib/Sema/SemaTemplateInstantiate.cpp
-+++ b/clang/lib/Sema/SemaTemplateInstantiate.cpp
-@@ -343,7 +343,7 @@
-       // If this function was instantiated from a specialized member that is
-       // a function template, we're done.
-       assert(FD->getPrimaryTemplate() && "No function template?");
--      if (FD->getPrimaryTemplate()->hasMemberSpecialization())
-+      if (FD->getPrimaryTemplate()->isMemberSpecialization())
-         return Done();
- 
-       // If this function is a generic lambda specialization, we are done.
-@@ -442,11 +442,11 @@
-         Specialized = CTSD->getSpecializedTemplateOrPartial();
-     if (auto *CTPSD =
-             Specialized.dyn_cast<ClassTemplatePartialSpecializationDecl *>()) {
--      if (CTPSD->hasMemberSpecialization())
-+      if (CTPSD->isMemberSpecialization())
-         return Done();
-     } else {
-       auto *CTD = Specialized.get<ClassTemplateDecl *>();
--      if (CTD->hasMemberSpecialization())
-+      if (CTD->isMemberSpecialization())
-         return Done();
-     }
-     return UseNextDecl(CTSD);
-@@ -478,11 +478,11 @@
-         Specialized = VTSD->getSpecializedTemplateOrPartial();
-     if (auto *VTPSD =
-             Specialized.dyn_cast<VarTemplatePartialSpecializationDecl *>()) {
--      if (VTPSD->hasMemberSpecialization())
-+      if (VTPSD->isMemberSpecialization())
-         return Done();
-     } else {
-       auto *VTD = Specialized.get<VarTemplateDecl *>();
--      if (VTD->hasMemberSpecialization())
-+      if (VTD->isMemberSpecialization())
-         return Done();
-     }
-     return UseNextDecl(VTSD);
-@@ -4141,7 +4141,7 @@
-   CXXRecordDecl *Pattern = nullptr;
-   Specialized = ClassTemplateSpec->getSpecializedTemplateOrPartial();
-   if (auto *CTD = Specialized.dyn_cast<ClassTemplateDecl *>()) {
--    while (!CTD->hasMemberSpecialization()) {
-+    while (!CTD->isMemberSpecialization()) {
-       if (auto *NewCTD = CTD->getInstantiatedFromMemberTemplate())
-         CTD = NewCTD;
-       else
-@@ -4151,7 +4151,7 @@
-   } else if (auto *CTPSD =
-                  Specialized
-                      .dyn_cast<ClassTemplatePartialSpecializationDecl *>()) {
--    while (!CTPSD->hasMemberSpecialization()) {
-+    while (!CTPSD->isMemberSpecialization()) {
-       if (auto *NewCTPSD = CTPSD->getInstantiatedFromMemberTemplate())
-         CTPSD = NewCTPSD;
-       else
-diff -ruN --strip-trailing-cr a/clang/test/AST/ast-dump-decl.cpp b/clang/test/AST/ast-dump-decl.cpp
---- a/clang/test/AST/ast-dump-decl.cpp
-+++ b/clang/test/AST/ast-dump-decl.cpp
-@@ -530,7 +530,7 @@
-   // CHECK-NEXT: |   `-ClassTemplateDecl 0x{{.+}} parent 0x{{.+}} <col:5, col:40> col:40 friend_undeclared TestClassTemplate{{$}}
-   // CHECK-NEXT: |     |-TemplateTypeParmDecl 0x{{.+}} <col:14, col:23> col:23 typename depth 1 index 0 T2{{$}}
-   // CHECK-NEXT: |     `-CXXRecordDecl 0x{{.+}} parent 0x{{.+}} <col:34, col:40> col:40 class TestClassTemplate{{$}}
--  // CHECK-NEXT: `-ClassTemplateSpecializationDecl 0x{{.+}} <line:[[@LINE-19]]:3, line:[[@LINE-17]]:3> line:[[@LINE-19]]:31 class TestClassTemplate definition implicit_instantiation{{$}}
-+  // CHECK-NEXT: `-ClassTemplateSpecializationDecl 0x{{.+}} <col:5, col:40> line:[[@LINE-19]]:31 class TestClassTemplate definition implicit_instantiation{{$}}
-   // CHECK-NEXT:   |-DefinitionData pass_in_registers empty aggregate standard_layout trivially_copyable pod trivial literal has_constexpr_non_copy_move_ctor can_const_default_init{{$}}
-   // CHECK-NEXT:   | |-DefaultConstructor exists trivial constexpr defaulted_is_constexpr{{$}}
-   // CHECK-NEXT:   | |-CopyConstructor simple trivial has_const_param implicit_has_const_param{{$}}
-diff -ruN --strip-trailing-cr a/clang/test/ASTMerge/class-template-spec/Inputs/class-template-spec.cpp b/clang/test/ASTMerge/class-template-spec/Inputs/class-template-spec.cpp
---- a/clang/test/ASTMerge/class-template-spec/Inputs/class-template-spec.cpp
-+++ b/clang/test/ASTMerge/class-template-spec/Inputs/class-template-spec.cpp
-@@ -0,0 +1,47 @@
-+namespace N0 {
-+  template<typename T>
-+  struct A {
-+    template<typename U>
-+    friend struct A;
-+  };
-+
-+  template struct A<long>;
-+} // namespace N0
-+
-+namespace N1 {
-+  template<typename T>
-+  struct A;
-+
-+  template<typename T>
-+  struct A {
-+    template<typename U>
-+    friend struct A;
-+  };
-+
-+  template struct A<long>;
-+} // namespace N1
-+
-+namespace N2 {
-+  template<typename T>
-+  struct A {
-+    template<typename U>
-+    friend struct A;
-+  };
-+
-+  template<typename T>
-+  struct A;
-+
-+  template struct A<long>;
-+} // namespace N2
-+
-+namespace N3 {
-+  struct A {
-+    template<typename T>
-+    friend struct B;
-+  };
-+
-+  template<typename T>
-+  struct B { };
-+
-+  template struct B<long>;
-+} // namespace N3
-diff -ruN --strip-trailing-cr a/clang/test/ASTMerge/class-template-spec/test.cpp b/clang/test/ASTMerge/class-template-spec/test.cpp
---- a/clang/test/ASTMerge/class-template-spec/test.cpp
-+++ b/clang/test/ASTMerge/class-template-spec/test.cpp
-@@ -0,0 +1,8 @@
-+// RUN: %clang_cc1 -emit-pch -o %t.1.ast %S/Inputs/class-template-spec.cpp
-+// RUN: %clang_cc1 -ast-merge %t.1.ast -fsyntax-only -verify %s
-+// expected-no-diagnostics
-+
-+template struct N0::A<short>;
-+template struct N1::A<short>;
-+template struct N2::A<short>;
-+template struct N3::B<short>;
-diff -ruN --strip-trailing-cr a/clang/test/CXX/temp/temp.spec/temp.expl.spec/p7.cpp b/clang/test/CXX/temp/temp.spec/temp.expl.spec/p7.cpp
---- a/clang/test/CXX/temp/temp.spec/temp.expl.spec/p7.cpp
-+++ b/clang/test/CXX/temp/temp.spec/temp.expl.spec/p7.cpp
-@@ -177,6 +177,93 @@
-   static_assert(A<short>::B<int*>::y == 2);
- } // namespace Defined
- 
-+namespace Constrained {
-+  template<typename T>
-+  struct A {
-+    template<typename U, bool V> requires V
-+    static constexpr int f(); // expected-note {{declared here}}
-+
-+    template<typename U, bool V> requires V
-+    static const int x; // expected-note {{declared here}}
-+
-+    template<typename U, bool V> requires V
-+    static const int x<U*, V>; // expected-note {{declared here}}
-+
-+    template<typename U, bool V> requires V
-+    struct B; // expected-note {{template is declared here}}
-+
-+    template<typename U, bool V> requires V
-+    struct B<U*, V>; // expected-note {{template is declared here}}
-+  };
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<short>::f() {
-+    return A<long>::f<U, V>();
-+  }
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<short>::x = A<long>::x<U, V>;
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<short>::x<U*, V> = A<long>::x<U*, V>;
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  struct A<short>::B<U*, V> {
-+    static constexpr int y = A<long>::B<U*, V>::y;
-+  };
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  struct A<short>::B {
-+    static constexpr int y = A<long>::B<U, V>::y;
-+  };
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<long>::f() {
-+    return 1;
-+  }
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<long>::x = 1;
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  constexpr int A<long>::x<U*, V> = 2;
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  struct A<long>::B {
-+    static constexpr int y = 1;
-+  };
-+
-+  template<>
-+  template<typename U, bool V> requires V
-+  struct A<long>::B<U*, V> {
-+    static constexpr int y = 2;
-+  };
-+
-+  static_assert(A<int>::f<int, true>() == 0); // expected-error {{static assertion expression is not an integral constant expression}}
-+                                              // expected-note@-1 {{undefined function 'f<int, true>' cannot be used in a constant expression}}
-+  static_assert(A<int>::x<int, true> == 0); // expected-error {{static assertion expression is not an integral constant expression}}
-+                                            // expected-note@-1 {{initializer of 'x<int, true>' is unknown}}
-+  static_assert(A<int>::x<int*, true> == 0); // expected-error {{static assertion expression is not an integral constant expression}}
-+                                             // expected-note@-1 {{initializer of 'x<int *, true>' is unknown}}
-+  static_assert(A<int>::B<int, true>::y == 0); // expected-error {{implicit instantiation of undefined template 'Constrained::A<int>::B<int, true>'}}
-+  static_assert(A<int>::B<int*, true>::y == 0); // expected-error {{implicit instantiation of undefined template 'Constrained::A<int>::B<int *, true>'}}
-+
-+  static_assert(A<short>::f<int, true>() == 1);
-+  static_assert(A<short>::x<int, true> == 1);
-+  static_assert(A<short>::x<int*, true> == 2);
-+  static_assert(A<short>::B<int, true>::y == 1);
-+  static_assert(A<short>::B<int*, true>::y == 2);
-+} // namespace Constrained
-+
- namespace Dependent {
-   template<int I>
-   struct A {
 diff -ruN --strip-trailing-cr a/clang/test/SemaCXX/nullability_redecl.cpp b/clang/test/SemaCXX/nullability_redecl.cpp
 --- a/clang/test/SemaCXX/nullability_redecl.cpp
 +++ b/clang/test/SemaCXX/nullability_redecl.cpp
@@ -588,14 +72,402 @@ diff -ruN --strip-trailing-cr a/clang/test/SemaCXX/nullability_redecl.cpp b/clan
 +  template<class T> class unique_ptr;
 +  using UP3 = unique_ptr<int> _Nonnull;
 +}
-diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
---- a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
-+++ b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
-@@ -3172,6 +3172,7 @@
-         ":TransformUtils",
-         ":Transforms",
-         ":VectorDialect",
-+        ":VectorTransforms",
-         "//llvm:Support",
-     ],
- )
+diff -ruN --strip-trailing-cr a/mlir/lib/Bindings/Python/IRAttributes.cpp b/mlir/lib/Bindings/Python/IRAttributes.cpp
+--- a/mlir/lib/Bindings/Python/IRAttributes.cpp
++++ b/mlir/lib/Bindings/Python/IRAttributes.cpp
+@@ -13,7 +13,6 @@
+ #include "IRModule.h"
+ 
+ #include "PybindUtils.h"
+-#include <pybind11/numpy.h>
+ 
+ #include "llvm/ADT/ScopeExit.h"
+ #include "llvm/Support/raw_ostream.h"
+@@ -758,10 +757,103 @@
+       throw py::error_already_set();
+     }
+     auto freeBuffer = llvm::make_scope_exit([&]() { PyBuffer_Release(&view); });
++    SmallVector<int64_t> shape;
++    if (explicitShape) {
++      shape.append(explicitShape->begin(), explicitShape->end());
++    } else {
++      shape.append(view.shape, view.shape + view.ndim);
++    }
+ 
++    MlirAttribute encodingAttr = mlirAttributeGetNull();
+     MlirContext context = contextWrapper->get();
+-    MlirAttribute attr = getAttributeFromBuffer(view, signless, explicitType,
+-                                                explicitShape, context);
++
++    // Detect format codes that are suitable for bulk loading. This includes
++    // all byte aligned integer and floating point types up to 8 bytes.
++    // Notably, this excludes, bool (which needs to be bit-packed) and
++    // other exotics which do not have a direct representation in the buffer
++    // protocol (i.e. complex, etc).
++    std::optional<MlirType> bulkLoadElementType;
++    if (explicitType) {
++      bulkLoadElementType = *explicitType;
++    } else {
++      std::string_view format(view.format);
++      if (format == "f") {
++        // f32
++        assert(view.itemsize == 4 && "mismatched array itemsize");
++        bulkLoadElementType = mlirF32TypeGet(context);
++      } else if (format == "d") {
++        // f64
++        assert(view.itemsize == 8 && "mismatched array itemsize");
++        bulkLoadElementType = mlirF64TypeGet(context);
++      } else if (format == "e") {
++        // f16
++        assert(view.itemsize == 2 && "mismatched array itemsize");
++        bulkLoadElementType = mlirF16TypeGet(context);
++      } else if (isSignedIntegerFormat(format)) {
++        if (view.itemsize == 4) {
++          // i32
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 32)
++                                    : mlirIntegerTypeSignedGet(context, 32);
++        } else if (view.itemsize == 8) {
++          // i64
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 64)
++                                    : mlirIntegerTypeSignedGet(context, 64);
++        } else if (view.itemsize == 1) {
++          // i8
++          bulkLoadElementType = signless ? mlirIntegerTypeGet(context, 8)
++                                         : mlirIntegerTypeSignedGet(context, 8);
++        } else if (view.itemsize == 2) {
++          // i16
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 16)
++                                    : mlirIntegerTypeSignedGet(context, 16);
++        }
++      } else if (isUnsignedIntegerFormat(format)) {
++        if (view.itemsize == 4) {
++          // unsigned i32
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 32)
++                                    : mlirIntegerTypeUnsignedGet(context, 32);
++        } else if (view.itemsize == 8) {
++          // unsigned i64
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 64)
++                                    : mlirIntegerTypeUnsignedGet(context, 64);
++        } else if (view.itemsize == 1) {
++          // i8
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 8)
++                                    : mlirIntegerTypeUnsignedGet(context, 8);
++        } else if (view.itemsize == 2) {
++          // i16
++          bulkLoadElementType = signless
++                                    ? mlirIntegerTypeGet(context, 16)
++                                    : mlirIntegerTypeUnsignedGet(context, 16);
++        }
++      }
++      if (!bulkLoadElementType) {
++        throw std::invalid_argument(
++            std::string("unimplemented array format conversion from format: ") +
++            std::string(format));
++      }
++    }
++
++    MlirType shapedType;
++    if (mlirTypeIsAShaped(*bulkLoadElementType)) {
++      if (explicitShape) {
++        throw std::invalid_argument("Shape can only be specified explicitly "
++                                    "when the type is not a shaped type.");
++      }
++      shapedType = *bulkLoadElementType;
++    } else {
++      shapedType = mlirRankedTensorTypeGet(shape.size(), shape.data(),
++                                           *bulkLoadElementType, encodingAttr);
++    }
++    size_t rawBufferSize = view.len;
++    MlirAttribute attr =
++        mlirDenseElementsAttrRawBufferGet(shapedType, rawBufferSize, view.buf);
+     if (mlirAttributeIsNull(attr)) {
+       throw std::invalid_argument(
+           "DenseElementsAttr could not be constructed from the given buffer. "
+@@ -871,13 +963,6 @@
+         // unsigned i16
+         return bufferInfo<uint16_t>(shapedType);
+       }
+-    } else if (mlirTypeIsAInteger(elementType) &&
+-               mlirIntegerTypeGetWidth(elementType) == 1) {
+-      // i1 / bool
+-      // We can not send the buffer directly back to Python, because the i1
+-      // values are bitpacked within MLIR. We call numpy's unpackbits function
+-      // to convert the bytes.
+-      return getBooleanBufferFromBitpackedAttribute();
+     }
+ 
+     // TODO: Currently crashes the program.
+@@ -931,183 +1016,14 @@
+            code == 'q';
+   }
+ 
+-  static MlirType
+-  getShapedType(std::optional<MlirType> bulkLoadElementType,
+-                std::optional<std::vector<int64_t>> explicitShape,
+-                Py_buffer &view) {
+-    SmallVector<int64_t> shape;
+-    if (explicitShape) {
+-      shape.append(explicitShape->begin(), explicitShape->end());
+-    } else {
+-      shape.append(view.shape, view.shape + view.ndim);
+-    }
+-
+-    if (mlirTypeIsAShaped(*bulkLoadElementType)) {
+-      if (explicitShape) {
+-        throw std::invalid_argument("Shape can only be specified explicitly "
+-                                    "when the type is not a shaped type.");
+-      }
+-      return *bulkLoadElementType;
+-    } else {
+-      MlirAttribute encodingAttr = mlirAttributeGetNull();
+-      return mlirRankedTensorTypeGet(shape.size(), shape.data(),
+-                                     *bulkLoadElementType, encodingAttr);
+-    }
+-  }
+-
+-  static MlirAttribute getAttributeFromBuffer(
+-      Py_buffer &view, bool signless, std::optional<PyType> explicitType,
+-      std::optional<std::vector<int64_t>> explicitShape, MlirContext &context) {
+-    // Detect format codes that are suitable for bulk loading. This includes
+-    // all byte aligned integer and floating point types up to 8 bytes.
+-    // Notably, this excludes exotics types which do not have a direct
+-    // representation in the buffer protocol (i.e. complex, etc).
+-    std::optional<MlirType> bulkLoadElementType;
+-    if (explicitType) {
+-      bulkLoadElementType = *explicitType;
+-    } else {
+-      std::string_view format(view.format);
+-      if (format == "f") {
+-        // f32
+-        assert(view.itemsize == 4 && "mismatched array itemsize");
+-        bulkLoadElementType = mlirF32TypeGet(context);
+-      } else if (format == "d") {
+-        // f64
+-        assert(view.itemsize == 8 && "mismatched array itemsize");
+-        bulkLoadElementType = mlirF64TypeGet(context);
+-      } else if (format == "e") {
+-        // f16
+-        assert(view.itemsize == 2 && "mismatched array itemsize");
+-        bulkLoadElementType = mlirF16TypeGet(context);
+-      } else if (format == "?") {
+-        // i1
+-        // The i1 type needs to be bit-packed, so we will handle it seperately
+-        return getBitpackedAttributeFromBooleanBuffer(view, explicitShape,
+-                                                      context);
+-      } else if (isSignedIntegerFormat(format)) {
+-        if (view.itemsize == 4) {
+-          // i32
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 32)
+-                                    : mlirIntegerTypeSignedGet(context, 32);
+-        } else if (view.itemsize == 8) {
+-          // i64
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 64)
+-                                    : mlirIntegerTypeSignedGet(context, 64);
+-        } else if (view.itemsize == 1) {
+-          // i8
+-          bulkLoadElementType = signless ? mlirIntegerTypeGet(context, 8)
+-                                         : mlirIntegerTypeSignedGet(context, 8);
+-        } else if (view.itemsize == 2) {
+-          // i16
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 16)
+-                                    : mlirIntegerTypeSignedGet(context, 16);
+-        }
+-      } else if (isUnsignedIntegerFormat(format)) {
+-        if (view.itemsize == 4) {
+-          // unsigned i32
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 32)
+-                                    : mlirIntegerTypeUnsignedGet(context, 32);
+-        } else if (view.itemsize == 8) {
+-          // unsigned i64
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 64)
+-                                    : mlirIntegerTypeUnsignedGet(context, 64);
+-        } else if (view.itemsize == 1) {
+-          // i8
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 8)
+-                                    : mlirIntegerTypeUnsignedGet(context, 8);
+-        } else if (view.itemsize == 2) {
+-          // i16
+-          bulkLoadElementType = signless
+-                                    ? mlirIntegerTypeGet(context, 16)
+-                                    : mlirIntegerTypeUnsignedGet(context, 16);
+-        }
+-      }
+-      if (!bulkLoadElementType) {
+-        throw std::invalid_argument(
+-            std::string("unimplemented array format conversion from format: ") +
+-            std::string(format));
+-      }
+-    }
+-
+-    MlirType type = getShapedType(bulkLoadElementType, explicitShape, view);
+-    return mlirDenseElementsAttrRawBufferGet(type, view.len, view.buf);
+-  }
+-
+-  // There is a complication for boolean numpy arrays, as numpy represents them
+-  // as 8 bits (1 byte) per boolean, whereas MLIR bitpacks them into 8 booleans
+-  // per byte.
+-  static MlirAttribute getBitpackedAttributeFromBooleanBuffer(
+-      Py_buffer &view, std::optional<std::vector<int64_t>> explicitShape,
+-      MlirContext &context) {
+-    if (llvm::endianness::native != llvm::endianness::little) {
+-      // Given we have no good way of testing the behavior on big-endian systems
+-      // we will throw
+-      throw py::type_error("Constructing a bit-packed MLIR attribute is "
+-                           "unsupported on big-endian systems");
+-    }
+-
+-    py::array_t<uint8_t> unpackedArray(view.len,
+-                                       static_cast<uint8_t *>(view.buf));
+-
+-    py::module numpy = py::module::import("numpy");
+-    py::object packbits_func = numpy.attr("packbits");
+-    py::object packed_booleans =
+-        packbits_func(unpackedArray, "bitorder"_a = "little");
+-    py::buffer_info pythonBuffer = packed_booleans.cast<py::buffer>().request();
+-
+-    MlirType bitpackedType =
+-        getShapedType(mlirIntegerTypeGet(context, 1), explicitShape, view);
+-    return mlirDenseElementsAttrRawBufferGet(bitpackedType, pythonBuffer.size,
+-                                             pythonBuffer.ptr);
+-  }
+-
+-  // This does the opposite transformation of
+-  // `getBitpackedAttributeFromBooleanBuffer`
+-  py::buffer_info getBooleanBufferFromBitpackedAttribute() {
+-    if (llvm::endianness::native != llvm::endianness::little) {
+-      // Given we have no good way of testing the behavior on big-endian systems
+-      // we will throw
+-      throw py::type_error("Constructing a numpy array from a MLIR attribute "
+-                           "is unsupported on big-endian systems");
+-    }
+-
+-    int64_t numBooleans = mlirElementsAttrGetNumElements(*this);
+-    int64_t numBitpackedBytes = llvm::divideCeil(numBooleans, 8);
+-    uint8_t *bitpackedData = static_cast<uint8_t *>(
+-        const_cast<void *>(mlirDenseElementsAttrGetRawData(*this)));
+-    py::array_t<uint8_t> packedArray(numBitpackedBytes, bitpackedData);
+-
+-    py::module numpy = py::module::import("numpy");
+-    py::object unpackbits_func = numpy.attr("unpackbits");
+-    py::object unpacked_booleans =
+-        unpackbits_func(packedArray, "bitorder"_a = "little");
+-    py::buffer_info pythonBuffer =
+-        unpacked_booleans.cast<py::buffer>().request();
+-
+-    MlirType shapedType = mlirAttributeGetType(*this);
+-    return bufferInfo<bool>(shapedType, (bool *)pythonBuffer.ptr, "?");
+-  }
+-
+   template <typename Type>
+   py::buffer_info bufferInfo(MlirType shapedType,
+                              const char *explicitFormat = nullptr) {
++    intptr_t rank = mlirShapedTypeGetRank(shapedType);
+     // Prepare the data for the buffer_info.
+-    // Buffer is configured for read-only access inside the `bufferInfo` call.
++    // Buffer is configured for read-only access below.
+     Type *data = static_cast<Type *>(
+         const_cast<void *>(mlirDenseElementsAttrGetRawData(*this)));
+-    return bufferInfo<Type>(shapedType, data, explicitFormat);
+-  }
+-
+-  template <typename Type>
+-  py::buffer_info bufferInfo(MlirType shapedType, Type *data,
+-                             const char *explicitFormat = nullptr) {
+-    intptr_t rank = mlirShapedTypeGetRank(shapedType);
+     // Prepare the shape for the buffer_info.
+     SmallVector<intptr_t, 4> shape;
+     for (intptr_t i = 0; i < rank; ++i)
+diff -ruN --strip-trailing-cr a/mlir/test/python/ir/array_attributes.py b/mlir/test/python/ir/array_attributes.py
+--- a/mlir/test/python/ir/array_attributes.py
++++ b/mlir/test/python/ir/array_attributes.py
+@@ -326,78 +326,6 @@
+         print(np.array(attr))
+ 
+ 
+-### 1 bit/boolean integer arrays
+-# CHECK-LABEL: TEST: testGetDenseElementsI1Signless
+-@run
+-def testGetDenseElementsI1Signless():
+-    with Context():
+-        array = np.array([True], dtype=np.bool_)
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK: dense<true> : tensor<1xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: [ True]
+-        print(np.array(attr))
+-
+-        array = np.array([[True, False, True], [True, True, False]], dtype=np.bool_)
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK{LITERAL}: dense<[[true, false, true], [true, true, false]]> : tensor<2x3xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: [[ True False True]
+-        # CHECK{LITERAL}:  [ True True False]]
+-        print(np.array(attr))
+-
+-        array = np.array(
+-            [[True, True, False, False], [True, False, True, False]], dtype=np.bool_
+-        )
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK{LITERAL}: dense<[[true, true, false, false], [true, false, true, false]]> : tensor<2x4xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: [[ True True False False]
+-        # CHECK{LITERAL}:  [ True False True False]]
+-        print(np.array(attr))
+-
+-        array = np.array(
+-            [
+-                [True, True, False, False],
+-                [True, False, True, False],
+-                [False, False, False, False],
+-                [True, True, True, True],
+-                [True, False, False, True],
+-            ],
+-            dtype=np.bool_,
+-        )
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK{LITERAL}: dense<[[true, true, false, false], [true, false, true, false], [false, false, false, false], [true, true, true, true], [true, false, false, true]]> : tensor<5x4xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: [[ True True False False]
+-        # CHECK{LITERAL}:  [ True False True False]
+-        # CHECK{LITERAL}:  [False False False False]
+-        # CHECK{LITERAL}:  [ True True True True]
+-        # CHECK{LITERAL}:  [ True False False True]]
+-        print(np.array(attr))
+-
+-        array = np.array(
+-            [
+-                [True, True, False, False, True, True, False, False, False],
+-                [False, False, False, True, False, True, True, False, True],
+-            ],
+-            dtype=np.bool_,
+-        )
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK{LITERAL}: dense<[[true, true, false, false, true, true, false, false, false], [false, false, false, true, false, true, true, false, true]]> : tensor<2x9xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: [[ True True False False True True False False False]
+-        # CHECK{LITERAL}:  [False False False True False True True False True]]
+-        print(np.array(attr))
+-
+-        array = np.array([], dtype=np.bool_)
+-        attr = DenseElementsAttr.get(array)
+-        # CHECK: dense<> : tensor<0xi1>
+-        print(attr)
+-        # CHECK{LITERAL}: []
+-        print(np.array(attr))
+-
+-
+ ### 16 bit integer arrays
+ # CHECK-LABEL: TEST: testGetDenseElementsI16Signless
+ @run
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index 7d0a20f..cbf4d37 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "308c00749ddb76b2e77934e986001b7fd4ad5cdc"
-    LLVM_SHA256 = "265d4d26d710110b7a3cb9379d1ece306ce46acbcfbdc61492da8772e299f066"
+    LLVM_COMMIT = "5b32c5954b1d00435a2264f8d1bd1fd9db9cb022"
+    LLVM_SHA256 = "c4b5566c7c4a419503d036b0c415a63c000fec302de3a32b872afdc70094fd93"
 
     tf_http_archive(
         name = name,
